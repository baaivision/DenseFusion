# <img src="figs/densefusion_icon.png" style="vertical-align: -10px;" :height="30px" width="30px">  DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception

Official pytorch implementation of **[DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception](http://arxiv.org/abs/2407.08303)**.
<p align="left">
   üìö <a href="https://arxiv.org/abs/2407.08303" target="_blank">Paper </a>ü§ó <a href="https://huggingface.co/datasets/BAAI/DenseFusion-1M" target="_blank">Dataset</a> 
</p>


- **Authors**: [Xiaotong Li](https://scholar.google.com/citations?user=cpCE_T4AAAAJ&hl=zh-CN), [Fan Zhang](https://scholar.google.com/citations?user=VsJ39HMAAAAJ), [Haiwen Diao](https://scholar.google.com/citations?user=46eCjHQAAAAJ&hl=zh-CN), [Yueze Wang](https://openreview.net/profile?id=~Yueze_Wang1), [Xinling Wang](https://scholar.google.com/citations?user=DPz0DjYAAAAJ&hl=zh-CN), [Ling-Yu Duan](https://scholar.google.com/citations?user=hsXZOgIAAAAJ&hl=zh-CN).
- **Institutes**:  Peking University; Beijing Academy of Artificial Intelligence; Dalian University of Technology
- **Dataset**: [ü§ó[DenseFusion-4V-100K](https://huggingface.co/datasets/BAAI/DenseFusion-1M/blob/main/DenseFusion-4V-100k.jsonl)], [ü§ó[DenseFusion-1M](https://huggingface.co/datasets/BAAI/DenseFusion-1M/blob/main/DenseFusion-1M.jsonl)]

## üìú News
[2024/07/12] The [paper](http://arxiv.org/abs/2407.08303) and [dataset](https://huggingface.co/datasets/BAAI/DenseFusion-1M) are released ! üí•   

## üí° Introduction
- *"An image is worth a thousand words"*. Comprehensive image descriptions are essential for multi-modal perception, while images contains various visual elements of different granularities that are challenging to harness.
- We propose **Perceptural Fusion** to integrate the diverse visual perception experts for capturing visual elements and adopt a MLLM as a centric pivot for comprehensive perception.
- We thereby provide **DenseFusion-1M** dataset for highly informative image descriptions with various visual details, including rich *OCR information*, *accurate object* and *position recognition*, and *external knowledge*, etc.

## üõ∏ Method
- Pipeline of *Perceptual Fusion* to acquire DenseFusion dataset with hyper-detailed image descriptions. This pipeline leverages various visual experts as image priors and employs a multimodal model as the central pivot for integrating multi-source information. Its capability is learned from a 100K meta dataset generated by advanced GPT-4V.
<p align="center">
      <img src="figs/fusion_process_method.png">
</p>


## üìö Dataset
- We carefully select 1M highly representative images from uncurated LAION dataset through *Semantic Clustering and De-duplication*.
- Through perceptual fusion, we obtain the comprehensive image-text data *DenseFusion-4V-100K* and *DenseFusion-1M*.
- You can download the dataset from the ü§ó[Huggingface](https://huggingface.co/datasets/BAAI/DenseFusion-1M) and images can be obtained from the urls using the `./download/download.py`.

|Dataset| Captioned by |Link|
|---|---|---|
|DenseFusion-4V-100K|GPT-4V|ü§ó[Huggingface](https://huggingface.co/datasets/BAAI/DenseFusion-1M)
|DenseFusion-1M|Ours|ü§ó[Huggingface](https://huggingface.co/datasets/BAAI/DenseFusion-1M)

- Visual examples from DenseFusion-1M, enriched with various detailed visual elements, such as *OCR information*, *object/attribute information*, *spaital position*, and *external world knowledge*.

<p align="center">
      <img src="figs/example.png">
</p>



## ü§ñ Benchmark Performance
We utilize this highly informative image captions DenseFusion-1M for *Pre-training Stage*. The training code largely follows [LLaVA](https://github.com/haotian-liu/LLaVA) and [ShareGPT4V](https://github.com/ShareGPT4Omni/ShareGPT4V).
- Low-resolution MLLM: [LLaVA](https://github.com/haotian-liu/LLaVA)
- High-resolution MLLM: [LLaVA-S<sup>2</sup>](https://github.com/bfshi/scaling_on_scales)

The high-quality image-text data brings consistent and significant improvements, especially for high-resolution MLLMs that require detailed visual information for effective learning.

| Model | LLM | SQA<sup>I | VQA<sup>v2 | GQA | VQA<sup>T| MME | MMB | SEED<sup>I | POPE | MMVet|
|---|---|---|---|---|---|---|---|---|---|---|
| LLaVA-7B | Vicuna_7B | 66.8 | 78.5 | 62.0 | 58.2 | 1510| 64.3 | 66.2 | 85.9 | 30.5 |
| DenseFusion-7B | Vicuna_7B | 69.3 | 80.8 | 64.0 | 62.0 | 1574 | 69.2 | 70.1 | 86.5 | 37.8 | 
| LLaVA-S<sup>2</sup>-7B | Vicuna_7B | 68.2 | 79.7 | 63.3 | 60.8 | 1520 | 66.4 | 67.2 | 86.7 | 34.6 |
| DenseFusion-S<sup>2</sup>-7B | Vicuna_7B | 72.1 | 81.6 | 65.3 | 67.4 | 1551 | 70.7 | 71.1 | 87.2 | 37.5| 



## ‚ù§Ô∏è Acknowledgments 
- [LLaVA](https://github.com/haotian-liu/LLaVA), [ShareGPT4V](https://github.com/ShareGPT4Omni/ShareGPT4V): Thanks for their wonderful works and code!
- [Vicuna](https://github.com/lm-sys/FastChat): The amazing open-sourced large language model series!
- [Scales on Scale: S<sup>2</sup>](https://github.com/bfshi/scaling_on_scales): The wonderful project for efficient and effective high-resolution MLLM architecture.
## ‚úíÔ∏è Citation 
If **DenseFusion** is helpful for your research, please consider **star** ‚≠ê and **citation** üìù :

```bibtex
@article{li2024DenseFusion,
      title={DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception}, 
      author={Xiaotong Li and Fan Zhang and Haiwen Diao and Yueze Wang and Xinlong Wang and Ling-Yu Duan},
      year={2024},
      journal={2407.08303},
```